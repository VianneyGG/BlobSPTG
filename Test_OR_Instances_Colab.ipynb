{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d71e66",
   "metadata": {},
   "source": [
    "# Steiner Tree Problem Solver: Testing OR Instances with Blob Simulation\n",
    "\n",
    "## Overview\n",
    "This notebook tests the Steiner Tree Problem solver using **Physarum polycephalum** (blob) simulation on the OR-Library test instances. The blob simulation mimics the behavior of the slime mold to find optimal Steiner trees in graphs.\n",
    "\n",
    "### What is the Steiner Tree Problem?\n",
    "Given a connected graph with weighted edges and a subset of vertices (terminals), find the minimum-weight tree that connects all terminals. Additional vertices (Steiner nodes) may be included to minimize the total weight.\n",
    "\n",
    "### The Blob Algorithm\n",
    "The algorithm simulates the growth patterns of *Physarum polycephalum*, which naturally optimizes network structures by:\n",
    "- Modeling edges as tubes carrying protoplasmic flux\n",
    "- Using pressure differentials to drive flow\n",
    "- Adapting tube conductivities based on usage\n",
    "- Converging to efficient network topologies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc33f12",
   "metadata": {},
   "source": [
    "## 1. Setup Google Colab Environment\n",
    "\n",
    "### Configure Google Colab\n",
    "First, let's configure the environment and check system information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "\n",
    "!git clone https://github.com/VianneyGG/BlobSPTG.git\n",
    "os.chdir('BlobSPTG')\n",
    "# Ensure the cloned repository is in the path\n",
    "sys.path.append(os.getcwd())\n",
    "# Display system information\n",
    "\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Enable GPU if available\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"GPU not available\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed\")\n",
    "\n",
    "# Check available memory\n",
    "import psutil\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe5f8c",
   "metadata": {},
   "source": [
    "## 2. Repository Setup and Verification\n",
    "\n",
    "### Verify Local Repository Structure\n",
    "We'll verify that the BlobSPTG repository is properly set up with all required files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9587199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we're in the correct repository directory\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Directory contents:\", sorted(os.listdir('.')))\n",
    "\n",
    "# Check if we're in the BlobSPTG repository\n",
    "required_files = ['MS3_PO_MT.py', 'test_evol_vs_smt.py', 'README.md']\n",
    "required_dirs = ['Fonctions', 'tests', 'media']\n",
    "\n",
    "print(\"\\nüîç Repository Verification:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check required files\n",
    "all_files_present = True\n",
    "for filename in required_files:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"‚úÖ {filename} - Found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {filename} - Missing\")\n",
    "        all_files_present = False\n",
    "\n",
    "# Check required directories\n",
    "all_dirs_present = True\n",
    "for dirname in required_dirs:\n",
    "    if os.path.exists(dirname) and os.path.isdir(dirname):\n",
    "        file_count = len([f for f in os.listdir(dirname) if os.path.isfile(os.path.join(dirname, f))])\n",
    "        print(f\"‚úÖ {dirname}/ - Found ({file_count} files)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dirname}/ - Missing\")\n",
    "        all_dirs_present = False\n",
    "\n",
    "if all_files_present and all_dirs_present:\n",
    "    print(\"\\nüéâ Repository structure verified successfully!\")\n",
    "    print(\"Ready to proceed with testing.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: Some required files or directories are missing.\")\n",
    "    print(\"Please ensure you're running this notebook from the BlobSPTG repository root.\")\n",
    "    \n",
    "# Display some basic repository information\n",
    "if os.path.exists('README.md'):\n",
    "    print(\"\\nüìã Repository Information:\")\n",
    "    with open('README.md', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:10]  # Read first 10 lines\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                print(f\"   {line.strip()}\")\n",
    "                if len([l for l in lines if l.strip()]) >= 3:  # Show first 3 non-empty lines\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43284bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify test instances from the local repository\n",
    "# The repository should contain all the OR-Library test instances\n",
    "\n",
    "print(\"Checking available test instances in local repository...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if os.path.exists('tests'):\n",
    "    # Get all test files\n",
    "    all_files = os.listdir('tests')\n",
    "    test_files = sorted([f for f in all_files if f.endswith('.txt') and f.startswith('stein')])\n",
    "    \n",
    "    print(f\"\\nüìù Found {len(test_files)} Steiner test files:\")\n",
    "    \n",
    "    # Group by instance type and analyze\n",
    "    instance_types = {}\n",
    "    for filename in test_files:\n",
    "        if len(filename) >= 7:  # steinX#.txt format\n",
    "            inst_type = filename[5]  # Extract 'b', 'c', 'd', 'e'\n",
    "            if inst_type not in instance_types:\n",
    "                instance_types[inst_type] = []\n",
    "            instance_types[inst_type].append(filename)\n",
    "    \n",
    "    total_instances = 0\n",
    "    print(\"\\nüìä Instance Distribution:\")\n",
    "    for inst_type in sorted(instance_types.keys()):\n",
    "        files = sorted(instance_types[inst_type])\n",
    "        count = len(files)\n",
    "        total_instances += count\n",
    "        print(f\"  Type {inst_type.upper()}: {count:2d} instances ({files[0]} to {files[-1]})\")\n",
    "    \n",
    "    print(f\"\\nüìà Summary Statistics:\")\n",
    "    print(f\"  Total test instances: {total_instances}\")\n",
    "    print(f\"  Instance types: {len(instance_types)} ({', '.join(sorted(instance_types.keys()))})\") \n",
    "    \n",
    "    # Show sample files for verification\n",
    "    print(f\"\\nüîç Sample test files (first 5):\")\n",
    "    for filename in test_files[:5]:\n",
    "        filepath = os.path.join('tests', filename)\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        print(f\"  - {filename} ({file_size} bytes)\")\n",
    "    \n",
    "    if len(test_files) > 5:\n",
    "        print(f\"  ... and {len(test_files) - 5} more files\")\n",
    "        \n",
    "    print(f\"\\n‚úÖ All OR-Library test instances are ready for testing!\")\n",
    "    \n",
    "    # Check for any non-test files in tests directory\n",
    "    other_files = [f for f in all_files if not (f.endswith('.txt') and f.startswith('stein'))]\n",
    "    if other_files:\n",
    "        print(f\"\\nüìÅ Other files in tests/ directory: {other_files}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: tests/ directory not found!\")\n",
    "    print(\"Please ensure you're running this notebook from the correct repository directory.\")\n",
    "    print(\"Available directories:\", [d for d in os.listdir('.') if os.path.isdir(d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4527c9c",
   "metadata": {},
   "source": [
    "## 3. Install Required Dependencies\n",
    "\n",
    "### Install Python Packages\n",
    "Let's install all the necessary packages for the blob simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the blob simulation\n",
    "!pip install numpy pandas matplotlib networkx tqdm psutil scipy\n",
    "\n",
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import heapq\n",
    "from math import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "\n",
    "print(\"All dependencies installed successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")\n",
    "\n",
    "# Verify repository structure and test module imports\n",
    "print(\"Checking repository modules and dependencies...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check main algorithm files\n",
    "main_files = ['MS3_PO_MT.py', 'test_evol_vs_smt.py']\n",
    "print(\"\\nüìÑ Main Algorithm Files:\")\n",
    "for filename in main_files:\n",
    "    if os.path.exists(filename):\n",
    "        file_size = os.path.getsize(filename)\n",
    "        print(f\"  ‚úÖ {filename} - Found ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {filename} - Missing\")\n",
    "\n",
    "# Check Fonctions directory modules\n",
    "print(\"\\nüîß Fonctions Module Directory:\")\n",
    "if os.path.exists('Fonctions'):\n",
    "    fonctions_files = [f for f in os.listdir('Fonctions') if f.endswith('.py') and not f.startswith('__')]\n",
    "    print(f\"  Found {len(fonctions_files)} Python modules:\")\n",
    "    for filename in sorted(fonctions_files):\n",
    "        print(f\"    - {filename}\")\n",
    "    \n",
    "    # Test importing from Fonctions\n",
    "    print(\"\\nüß™ Testing module imports:\")\n",
    "    sys.path.insert(0, 'Fonctions')\n",
    "    \n",
    "    test_modules = ['Tools', 'Initialisation', 'Pression', 'Update']\n",
    "    imported_modules = []\n",
    "    \n",
    "    for module_name in test_modules:\n",
    "        try:\n",
    "            module = __import__(module_name)\n",
    "            print(f\"  ‚úÖ {module_name} - Import successful\")\n",
    "            imported_modules.append(module_name)\n",
    "        except ImportError as e:\n",
    "            print(f\"  ‚ö†Ô∏è  {module_name} - Import failed: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {module_name} - Error: {e}\")\n",
    "    \n",
    "    if imported_modules:\n",
    "        print(f\"\\n‚úÖ Successfully imported {len(imported_modules)}/{len(test_modules)} modules\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No modules could be imported - may need to install dependencies\")\n",
    "else:\n",
    "    print(\"  ‚ùå Fonctions/ directory not found\")\n",
    "\n",
    "# Check test instances one more time\n",
    "print(\"\\nüìä Test Instances Summary:\")\n",
    "if os.path.exists('tests'):\n",
    "    test_count = len([f for f in os.listdir('tests') if f.endswith('.txt') and f.startswith('stein')])\n",
    "    print(f\"  ‚úÖ {test_count} Steiner test instances available\")\n",
    "else:\n",
    "    print(\"  ‚ùå No test instances found\")\n",
    "\n",
    "print(\"\\nüöÄ Repository verification complete!\")\n",
    "print(\"Ready to install Python dependencies and run tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for the BlobSPTG repository\n",
    "print(\"Installing required packages...\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Install packages that are commonly needed\n",
    "packages = [\n",
    "    'numpy',           # Numerical computations\n",
    "    'pandas',          # Data analysis and results handling  \n",
    "    'matplotlib',      # Plotting and visualization\n",
    "    'networkx',        # Graph algorithms and structures\n",
    "    'tqdm',           # Progress bars\n",
    "    'psutil',         # System monitoring\n",
    "    'scipy'           # Scientific computing\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    \n",
    "try:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Install all packages at once\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install'] + packages,\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ All packages installed successfully!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Installation warning: {result.stderr}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Installation error: {e}\")\n",
    "    print(\"Trying alternative installation method...\")\n",
    "    \n",
    "    # Fallback: install one by one\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"‚úÖ {package} already available\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            os.system(f'{sys.executable} -m pip install {package}')\n",
    "\n",
    "# Import and verify all required libraries\n",
    "print(\"\\nüì¶ Verifying imports:\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "    from tqdm.auto import tqdm\n",
    "    import time\n",
    "    import heapq\n",
    "    from math import *\n",
    "    import psutil\n",
    "    \n",
    "    print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "    print(f\"‚úÖ Pandas version: {pd.__version__}\")\n",
    "    print(f\"‚úÖ NetworkX version: {nx.__version__}\")\n",
    "    print(f\"‚úÖ Matplotlib backend: {plt.get_backend()}\")\n",
    "    print(f\"‚úÖ All core libraries imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Some packages may need manual installation.\")\n",
    "\n",
    "# Check system resources\n",
    "print(\"\\nüíª System Resources:\")\n",
    "try:\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"  Available RAM: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"  CPU cores: {psutil.cpu_count()}\")\n",
    "except:\n",
    "    print(\"  System resource information unavailable\")\n",
    "\n",
    "print(\"\\nüéØ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a418509",
   "metadata": {},
   "source": [
    "## 4. Implement the Blob Algorithm\n",
    "\n",
    "### Core Algorithm Functions\n",
    "Let's implement the essential functions for the blob simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca93a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for parsing Steiner tree instances\n",
    "def parse_stein_file(filepath):\n",
    "    \"\"\"Parse a steinX.txt file and extract graph information.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    n_vertices, n_edges = map(int, lines[0].split())\n",
    "    edge_lines = lines[1:1+n_edges]\n",
    "    edges = []\n",
    "    for line in edge_lines:\n",
    "        u, v, cost = map(int, line.split())\n",
    "        edges.append((u, v, cost))\n",
    "    \n",
    "    n_terminals = int(lines[1+n_edges])\n",
    "    # Read terminals from all remaining lines\n",
    "    terminal_lines = lines[2+n_edges:]\n",
    "    terminals = []\n",
    "    for line in terminal_lines:\n",
    "        terminals.extend(map(int, line.split()))\n",
    "    \n",
    "    return n_vertices, n_edges, edges, terminals\n",
    "\n",
    "def build_graph(n_vertices, edges):\n",
    "    \"\"\"Build adjacency matrix from edge list.\"\"\"\n",
    "    G = np.full((n_vertices, n_vertices), np.inf)\n",
    "    for u, v, cost in edges:\n",
    "        G[u-1, v-1] = cost\n",
    "        G[v-1, u-1] = cost\n",
    "    return G\n",
    "\n",
    "print(\"Helper functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Blob Algorithm Implementation\n",
    "def simple_blob_steiner(G, terminals, max_iterations=100, alpha=0.15, mu=1.0, delta=0.1):\n",
    "    \"\"\"\n",
    "    Simplified blob simulation for Steiner Tree Problem.\n",
    "    \n",
    "    Args:\n",
    "        G: Adjacency matrix (numpy array)\n",
    "        terminals: Set of terminal indices\n",
    "        max_iterations: Maximum number of iterations\n",
    "        alpha: Learning rate for conductivity updates\n",
    "        mu: Flux strength parameter\n",
    "        delta: Minimum conductivity threshold\n",
    "    \n",
    "    Returns:\n",
    "        Final adjacency matrix with selected edges\n",
    "    \"\"\"\n",
    "    n = G.shape[0]\n",
    "    terminals = list(terminals)\n",
    "    \n",
    "    # Initialize conductivities\n",
    "    D = np.ones_like(G) * 0.1\n",
    "    D[np.isinf(G)] = 0\n",
    "    \n",
    "    # Initialize pressures\n",
    "    pressures = np.zeros(n)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Set boundary conditions (terminals as sources/sinks)\n",
    "        if len(terminals) >= 2:\n",
    "            pressures[terminals[0]] = 1.0  # Source\n",
    "            pressures[terminals[-1]] = 0.0  # Sink\n",
    "        \n",
    "        # Calculate flows using conductivities\n",
    "        flows = np.zeros_like(G)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if not np.isinf(G[i, j]) and D[i, j] > 0:\n",
    "                    flow = D[i, j] * (pressures[i] - pressures[j]) / G[i, j]\n",
    "                    flows[i, j] = flow\n",
    "                    flows[j, i] = -flow\n",
    "        \n",
    "        # Update pressures (simplified pressure calculation)\n",
    "        new_pressures = pressures.copy()\n",
    "        for i in range(n):\n",
    "            if i not in terminals:  # Only update non-terminal nodes\n",
    "                total_flow = 0\n",
    "                total_conductivity = 0\n",
    "                for j in range(n):\n",
    "                    if not np.isinf(G[i, j]) and D[i, j] > 0:\n",
    "                        total_flow += D[i, j] * pressures[j] / G[i, j]\n",
    "                        total_conductivity += D[i, j] / G[i, j]\n",
    "                \n",
    "                if total_conductivity > 0:\n",
    "                    new_pressures[i] = total_flow / total_conductivity\n",
    "        \n",
    "        pressures = new_pressures\n",
    "        \n",
    "        # Update conductivities based on flow\n",
    "        new_D = D.copy()\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if not np.isinf(G[i, j]):\n",
    "                    flow_magnitude = abs(flows[i, j])\n",
    "                    # Reinforcement: increase conductivity for high-flow edges\n",
    "                    new_D[i, j] = max(delta, D[i, j] * (1 + alpha * flow_magnitude))\n",
    "                    new_D[j, i] = new_D[i, j]\n",
    "        \n",
    "        # Apply decay to unused edges\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if not np.isinf(G[i, j]):\n",
    "                    if abs(flows[i, j]) < 0.01:  # Low flow threshold\n",
    "                        new_D[i, j] *= (1 - mu * delta)\n",
    "                        new_D[j, i] = new_D[i, j]\n",
    "        \n",
    "        D = new_D\n",
    "        \n",
    "        # Check convergence (simplified)\n",
    "        if iteration % 20 == 0:\n",
    "            print(f\"Iteration {iteration}: Active edges = {np.sum(D > delta)}\")\n",
    "    \n",
    "    # Extract final tree (edges with significant conductivity)\n",
    "    result = np.full_like(G, np.inf)\n",
    "    threshold = np.max(D) * 0.1  # Adaptive threshold\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if D[i, j] > threshold and not np.isinf(G[i, j]):\n",
    "                result[i, j] = G[i, j]\n",
    "                result[j, i] = G[i, j]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Blob algorithm implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa52aed",
   "metadata": {},
   "source": [
    "## 5. Run Tests on OR Instances\n",
    "\n",
    "### Test the Blob Algorithm\n",
    "Now let's test our blob simulation on the OR-Library instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the actual MS3_PO_MT function from the local repository\n",
    "print(\"Importing algorithm from local repository...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Add the current directory to Python path for imports\n",
    "import sys\n",
    "if '.' not in sys.path:\n",
    "    sys.path.insert(0, '.')\n",
    "if './Fonctions' not in sys.path:\n",
    "    sys.path.insert(0, './Fonctions')\n",
    "\n",
    "# Try to import the main algorithm\n",
    "MS3_PO_MT = None\n",
    "algorithm_source = \"None\"\n",
    "\n",
    "try:\n",
    "    from MS3_PO_MT import MS3_PO_MT\n",
    "    print(\"‚úÖ Successfully imported MS3_PO_MT from repository\")\n",
    "    algorithm_source = \"Repository MS3_PO_MT\"\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to import MS3_PO_MT: {e}\")\n",
    "    print(\"Will use simplified blob algorithm as fallback\")\n",
    "    algorithm_source = \"Simplified Fallback\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error importing MS3_PO_MT: {e}\")\n",
    "    algorithm_source = \"Simplified Fallback\"\n",
    "\n",
    "# Try to import supporting modules\n",
    "print(\"\\nüîß Importing supporting modules:\")\n",
    "supporting_modules = []\n",
    "module_names = ['Tools', 'Initialisation', 'Pression', 'Update', 'Evolution']\n",
    "\n",
    "for module_name in module_names:\n",
    "    try:\n",
    "        module = __import__(module_name)\n",
    "        supporting_modules.append(module_name)\n",
    "        print(f\"  ‚úÖ {module_name} imported successfully\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ö†Ô∏è {module_name} not available\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {module_name} error: {e}\")\n",
    "\n",
    "print(f\"\\nImported {len(supporting_modules)}/{len(module_names)} supporting modules\")\n",
    "\n",
    "# Load known optimal solutions from the test_evol_vs_smt.py patterns\n",
    "# These are the benchmark values for comparison\n",
    "SMT_OPTIMAL = {\n",
    "    'b': {1: 82.0, 2: 83.0, 3: 138.0, 4: 59.0, 5: 61.0, 6: 122.0, 7: 111.0, 8: 104.0, \n",
    "          9: 220.0, 10: 86.0, 11: 88.0, 12: 174.0, 13: 165.0, 14: 235.0, 15: 318.0, \n",
    "          16: 127.0, 17: 131.0, 18: 218.0},\n",
    "    'c': {1: 85.0, 2: 144.0, 3: 754.0, 4: 1079.0, 5: 1579.0, 6: 55.0, 7: 102.0, \n",
    "          8: 509.0, 9: 707.0, 10: 1093.0, 11: 32.0, 12: 46.0, 13: 258.0, 14: 323.0, \n",
    "          15: 556.0, 16: 11.0, 17: 18.0, 18: 113.0, 19: 146.0, 20: 267.0},\n",
    "    'd': {1: 106.0, 2: 220.0, 3: 1565.0, 4: 1935.0, 5: 3250.0, 6: 67.0, 7: 103.0, \n",
    "          8: 1072.0, 9: 1448.0, 10: 2110.0, 11: 29.0, 12: 42.0, 13: 500.0, 14: 667.0, \n",
    "          15: 1116.0, 16: 13.0, 17: 23.0, 18: 223.0, 19: 310.0, 20: 537.0},\n",
    "    'e': {1: 111.0, 2: 214.0, 3: 4013.0, 4: 5101.0, 5: 8128.0, 6: 73.0, 7: 145.0, \n",
    "          8: 2640.0, 9: 3604.0, 10: 5600.0, 11: 34.0, 12: 67.0, 13: 1280.0, 14: 1732.0, \n",
    "          15: 2784.0, 16: 15.0, 17: 25.0, 18: 564.0, 19: 758.0, 20: 1342.0}\n",
    "}\n",
    "\n",
    "def run_test_on_instance(filename, use_repository_algorithm=True, verbose=True):\n",
    "    \"\"\"Run blob algorithm on a single test instance from the local repository.\"\"\"\n",
    "    filepath = os.path.join('tests', filename)\n",
    "    if not os.path.isfile(filepath):\n",
    "        if verbose:\n",
    "            print(f\"‚ùå File not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Parse the instance\n",
    "        n_vertices, n_edges, edges, terminals = parse_stein_file(filepath)\n",
    "        G = build_graph(n_vertices, edges)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Testing {filename} ---\")\n",
    "            print(f\"Vertices: {n_vertices}, Edges: {n_edges}, Terminals: {len(terminals)}\")\n",
    "            print(f\"Terminal nodes: {terminals[:10]}{'...' if len(terminals) > 10 else ''}\")\n",
    "        \n",
    "        # Convert terminals to 0-based indexing\n",
    "        terminal_set = set([t-1 for t in terminals])\n",
    "        \n",
    "        # Run algorithm\n",
    "        start_time = time.time()\n",
    "        algorithm_used = \"Unknown\"\n",
    "        \n",
    "        if use_repository_algorithm and MS3_PO_MT is not None:\n",
    "            # Use the actual repository algorithm\n",
    "            try:\n",
    "                result_matrix = MS3_PO_MT(G, terminal_set,\n",
    "                                        M=50,  # Number of iterations\n",
    "                                        K=10,  # Population size  \n",
    "                                        alpha=0.15,\n",
    "                                        mu=1,\n",
    "                                        delta=0.1,\n",
    "                                        S=3,\n",
    "                                        √©vol=False,\n",
    "                                        modeRenfo='vieillesse',\n",
    "                                        modeProba='weighted')\n",
    "                algorithm_used = \"Repository MS3_PO_MT\"\n",
    "            except Exception as repo_error:\n",
    "                if verbose:\n",
    "                    print(f\"Repository algorithm failed: {repo_error}\")\n",
    "                    print(\"Falling back to simplified algorithm...\")\n",
    "                result_matrix = simple_blob_steiner(G, terminal_set, max_iterations=50)\n",
    "                algorithm_used = \"Simplified Blob (fallback)\"\n",
    "        else:\n",
    "            # Use simplified algorithm\n",
    "            result_matrix = simple_blob_steiner(G, terminal_set, max_iterations=50)\n",
    "            algorithm_used = \"Simplified Blob\"\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate solution weight\n",
    "        if result_matrix is not None:\n",
    "            mask = np.isfinite(result_matrix)\n",
    "            solution_weight = np.sum(result_matrix[mask]) / 2  # Divide by 2 for undirected graph\n",
    "        else:\n",
    "            solution_weight = float('inf')\n",
    "        \n",
    "        # Get optimal solution for comparison\n",
    "        instance_type = filename[5] if len(filename) > 5 else 'unknown'\n",
    "        try:\n",
    "            instance_num = int(''.join(filter(str.isdigit, filename.split('.')[0][6:])))\n",
    "        except:\n",
    "            instance_num = 0\n",
    "            \n",
    "        optimal_weight = SMT_OPTIMAL.get(instance_type, {}).get(instance_num, float('inf'))\n",
    "        \n",
    "        # Calculate error percentage\n",
    "        if optimal_weight != float('inf') and optimal_weight > 0:\n",
    "            error_pct = max(0, (solution_weight - optimal_weight) / optimal_weight * 100)\n",
    "        else:\n",
    "            error_pct = float('inf')\n",
    "        \n",
    "        runtime = end_time - start_time\n",
    "        \n",
    "        result = {\n",
    "            'file': filename,\n",
    "            'instance_type': instance_type,\n",
    "            'instance_num': instance_num,\n",
    "            'vertices': n_vertices,\n",
    "            'edges': n_edges,\n",
    "            'terminals': len(terminals),\n",
    "            'blob_weight': solution_weight,\n",
    "            'optimal_weight': optimal_weight,\n",
    "            'error_pct': error_pct,\n",
    "            'runtime': runtime,\n",
    "            'algorithm': algorithm_used\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Algorithm: {algorithm_used}\")\n",
    "            print(f\"Blob solution: {solution_weight:.2f}\")\n",
    "            print(f\"Optimal: {optimal_weight:.2f}\")\n",
    "            print(f\"Error: {error_pct:.2f}%\")\n",
    "            print(f\"Runtime: {runtime:.3f}s\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(f\"\\nüéØ Enhanced test function ready!\")\n",
    "print(f\"Primary algorithm source: {algorithm_source}\")\n",
    "print(f\"Supporting modules available: {len(supporting_modules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f606fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests on OR instances from the local repository\n",
    "results = []\n",
    "\n",
    "print(\"Preparing to test OR instances from local repository\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if os.path.exists('tests'):\n",
    "    # Get all available test files\n",
    "    all_test_files = sorted([f for f in os.listdir('tests') \n",
    "                            if f.endswith('.txt') and f.startswith('stein')])\n",
    "    \n",
    "    print(f\"\\nüìä Found {len(all_test_files)} test files in local repository\")\n",
    "    \n",
    "    # Group by instance type for organized testing\n",
    "    instance_groups = {}\n",
    "    for filename in all_test_files:\n",
    "        if len(filename) >= 7:\n",
    "            inst_type = filename[5]  # 'b', 'c', 'd', 'e'\n",
    "            if inst_type not in instance_groups:\n",
    "                instance_groups[inst_type] = []\n",
    "            instance_groups[inst_type].append(filename)\n",
    "    \n",
    "    print(\"\\nüìã Available instance types:\")\n",
    "    for inst_type, files in sorted(instance_groups.items()):\n",
    "        print(f\"  Type {inst_type.upper()}: {len(files)} instances\")\n",
    "    \n",
    "    # Configuration: Choose testing strategy\n",
    "    TEST_ALL = False  # Set to True to test all instances\n",
    "    INSTANCES_PER_TYPE = 2  # Number of instances to test per type (if not testing all)\n",
    "    \n",
    "    if TEST_ALL:\n",
    "        test_files = all_test_files\n",
    "        print(f\"\\nüöÄ TESTING ALL {len(test_files)} INSTANCES\")\n",
    "    else:\n",
    "        # Test a representative subset\n",
    "        test_files = []\n",
    "        print(f\"\\nüéØ TESTING SUBSET: {INSTANCES_PER_TYPE} instances per type\")\n",
    "        \n",
    "        for inst_type in sorted(instance_groups.keys()):\n",
    "            type_files = sorted(instance_groups[inst_type])\n",
    "            # Take first few and last few for variety\n",
    "            selected = type_files[:INSTANCES_PER_TYPE//2] + type_files[-(INSTANCES_PER_TYPE//2):]\n",
    "            test_files.extend(selected[:INSTANCES_PER_TYPE])  # Ensure we don't exceed limit\n",
    "            \n",
    "        test_files = sorted(list(set(test_files)))  # Remove duplicates and sort\n",
    "    \n",
    "    print(f\"Selected {len(test_files)} files for testing:\")\n",
    "    for i, filename in enumerate(test_files):\n",
    "        print(f\"  {i+1:2d}. {filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING BLOB TESTS ON OR INSTANCES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run tests with progress tracking\n",
    "    successful_tests = 0\n",
    "    failed_tests = 0\n",
    "    \n",
    "    for i, filename in enumerate(test_files):\n",
    "        print(f\"\\n[{i+1}/{len(test_files)}] Testing {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            result = run_test_on_instance(filename, \n",
    "                                        use_repository_algorithm=True, \n",
    "                                        verbose=True)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                successful_tests += 1\n",
    "                \n",
    "                # Quick status update\n",
    "                if result['error_pct'] != float('inf'):\n",
    "                    status = \"üü¢\" if result['error_pct'] <= 5 else \"üü°\" if result['error_pct'] <= 15 else \"üî¥\"\n",
    "                    print(f\"Status: {status} ({result['error_pct']:.1f}% error)\")\n",
    "                else:\n",
    "                    print(\"Status: ‚ùì (no reference solution)\")\n",
    "            else:\n",
    "                failed_tests += 1\n",
    "                print(\"‚ùå Test failed\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚èπÔ∏è Testing interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            failed_tests += 1\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TESTING COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Successful tests: {successful_tests}\")\n",
    "    print(f\"‚ùå Failed tests: {failed_tests}\")\n",
    "    print(f\"üìä Total results collected: {len(results)}\")\n",
    "    \n",
    "    if results:\n",
    "        # Quick preview of results\n",
    "        finite_errors = [r['error_pct'] for r in results if r['error_pct'] != float('inf')]\n",
    "        if finite_errors:\n",
    "            avg_error = sum(finite_errors) / len(finite_errors)\n",
    "            print(f\"üìà Average error: {avg_error:.2f}%\")\n",
    "            print(f\"üìà Best error: {min(finite_errors):.2f}%\")\n",
    "            print(f\"üìà Worst error: {max(finite_errors):.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: tests/ directory not found!\")\n",
    "    print(\"Please ensure you're running this notebook from the BlobSPTG repository root.\")\n",
    "    print(\"Available directories:\", [d for d in os.listdir('.') if os.path.isdir(d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcb1cd",
   "metadata": {},
   "source": [
    "## 6. Analyze Test Results\n",
    "\n",
    "### Results Summary and Visualization\n",
    "Let's analyze the performance of our blob algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da04314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results analysis\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Display results table with better formatting\n",
    "    print(\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Format the display DataFrame\n",
    "    display_df = df_results.copy()\n",
    "    \n",
    "    # Round numerical columns for better display\n",
    "    numeric_cols = ['blob_weight', 'optimal_weight', 'error_pct', 'runtime']\n",
    "    for col in numeric_cols:\n",
    "        if col in display_df.columns:\n",
    "            display_df[col] = display_df[col].round(3)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = ['file', 'instance_type', 'vertices', 'edges', 'terminals', \n",
    "                   'blob_weight', 'optimal_weight', 'error_pct', 'runtime', 'algorithm']\n",
    "    display_cols = [col for col in column_order if col in display_df.columns]\n",
    "    display_df = display_df[display_cols]\n",
    "    \n",
    "    # Set pandas display options\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 20)\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Statistical Analysis\n",
    "    print(\"\\n\\nüìà PERFORMANCE STATISTICS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    finite_errors = df_results[df_results['error_pct'] != float('inf')]['error_pct']\n",
    "    \n",
    "    if not finite_errors.empty:\n",
    "        print(f\"Error Analysis ({len(finite_errors)} instances):\")\n",
    "        print(f\"  Mean error:      {finite_errors.mean():.2f}%\")\n",
    "        print(f\"  Median error:    {finite_errors.median():.2f}%\")\n",
    "        print(f\"  Std deviation:   {finite_errors.std():.2f}%\")\n",
    "        print(f\"  Min error:       {finite_errors.min():.2f}%\")\n",
    "        print(f\"  Max error:       {finite_errors.max():.2f}%\")\n",
    "        \n",
    "        # Quality categorization\n",
    "        excellent = len(finite_errors[finite_errors <= 2])    # Within 2%\n",
    "        good = len(finite_errors[finite_errors <= 5])         # Within 5% \n",
    "        acceptable = len(finite_errors[finite_errors <= 10])  # Within 10%\n",
    "        poor = len(finite_errors[finite_errors > 10])         # Over 10%\n",
    "        \n",
    "        print(f\"\\nQuality Breakdown:\")\n",
    "        print(f\"  Excellent (‚â§2%):   {excellent:2d}/{len(finite_errors)} ({excellent/len(finite_errors)*100:.1f}%)\")\n",
    "        print(f\"  Good (‚â§5%):        {good:2d}/{len(finite_errors)} ({good/len(finite_errors)*100:.1f}%)\")\n",
    "        print(f\"  Acceptable (‚â§10%): {acceptable:2d}/{len(finite_errors)} ({acceptable/len(finite_errors)*100:.1f}%)\")\n",
    "        print(f\"  Poor (>10%):       {poor:2d}/{len(finite_errors)} ({poor/len(finite_errors)*100:.1f}%)\")\n",
    "    \n",
    "    # Runtime Analysis\n",
    "    print(f\"\\nRuntime Analysis:\")\n",
    "    print(f\"  Mean runtime:    {df_results['runtime'].mean():.3f}s\")\n",
    "    print(f\"  Median runtime:  {df_results['runtime'].median():.3f}s\")\n",
    "    print(f\"  Min runtime:     {df_results['runtime'].min():.3f}s\")\n",
    "    print(f\"  Max runtime:     {df_results['runtime'].max():.3f}s\")\n",
    "    print(f\"  Total time:      {df_results['runtime'].sum():.2f}s\")\n",
    "    \n",
    "    # Instance Type Analysis\n",
    "    if 'instance_type' in df_results.columns:\n",
    "        print(f\"\\nInstance Type Performance:\")\n",
    "        type_analysis = df_results.groupby('instance_type').agg({\n",
    "            'error_pct': ['count', 'mean', 'min', 'max'],\n",
    "            'runtime': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        for inst_type in sorted(df_results['instance_type'].unique()):\n",
    "            type_data = df_results[df_results['instance_type'] == inst_type]\n",
    "            type_errors = type_data[type_data['error_pct'] != float('inf')]['error_pct']\n",
    "            \n",
    "            if not type_errors.empty:\n",
    "                print(f\"  Type {inst_type.upper()}: {len(type_data)} tests, \"\n",
    "                      f\"avg error {type_errors.mean():.2f}%, \"\n",
    "                      f\"avg runtime {type_data['runtime'].mean():.3f}s\")\n",
    "    \n",
    "    # Algorithm Usage\n",
    "    if 'algorithm' in df_results.columns:\n",
    "        print(f\"\\nAlgorithm Usage:\")\n",
    "        algo_counts = df_results['algorithm'].value_counts()\n",
    "        for algo, count in algo_counts.items():\n",
    "            print(f\"  {algo}: {count} instances ({count/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Best and Worst Performers\n",
    "    if not finite_errors.empty:\n",
    "        best_idx = df_results[df_results['error_pct'] == finite_errors.min()].index[0]\n",
    "        worst_idx = df_results[df_results['error_pct'] == finite_errors.max()].index[0]\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Performance:\")\n",
    "        best = df_results.loc[best_idx]\n",
    "        print(f\"  {best['file']}: {best['error_pct']:.2f}% error in {best['runtime']:.3f}s\")\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è Worst Performance:\")\n",
    "        worst = df_results.loc[worst_idx]\n",
    "        print(f\"  {worst['file']}: {worst['error_pct']:.2f}% error in {worst['runtime']:.3f}s\")\n",
    "    \n",
    "    # Save results with timestamp\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f'blob_steiner_results_{timestamp}.csv'\n",
    "    df_results.to_csv(filename, index=False)\n",
    "    print(f\"\\nüíæ Results saved to '{filename}'\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis complete! Tested {len(results)} instances successfully.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results available for analysis.\")\n",
    "    print(\"Please run the test execution cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "if results and len(results) > 0:\n",
    "    df_viz = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"Creating comprehensive visualization suite...\")\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('BlobSPTG Algorithm Performance Analysis on OR-Library Instances', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Error Distribution Histogram\n",
    "    finite_errors = df_viz[df_viz['error_pct'] != float('inf')]['error_pct']\n",
    "    if not finite_errors.empty:\n",
    "        axes[0, 0].hist(finite_errors, bins=min(15, len(finite_errors)), \n",
    "                       alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].axvline(finite_errors.mean(), color='red', linestyle='--', \n",
    "                          label=f'Mean: {finite_errors.mean():.2f}%')\n",
    "        axes[0, 0].axvline(finite_errors.median(), color='orange', linestyle='--', \n",
    "                          label=f'Median: {finite_errors.median():.2f}%')\n",
    "        axes[0, 0].set_title('Error Percentage Distribution')\n",
    "        axes[0, 0].set_xlabel('Error (%)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Solution Quality Scatter Plot\n",
    "    valid_data = df_viz[df_viz['optimal_weight'] != float('inf')]\n",
    "    if not valid_data.empty:\n",
    "        scatter = axes[0, 1].scatter(valid_data['optimal_weight'], valid_data['blob_weight'], \n",
    "                                   alpha=0.7, c=valid_data['error_pct'], cmap='RdYlGn_r', s=60)\n",
    "        \n",
    "        # Add perfect solution line\n",
    "        min_val = min(valid_data['optimal_weight'].min(), valid_data['blob_weight'].min())\n",
    "        max_val = max(valid_data['optimal_weight'].max(), valid_data['blob_weight'].max())\n",
    "        axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='Perfect')\n",
    "        \n",
    "        axes[0, 1].set_title('Blob vs Optimal Solutions')\n",
    "        axes[0, 1].set_xlabel('Optimal Weight')\n",
    "        axes[0, 1].set_ylabel('Blob Solution Weight')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[0, 1])\n",
    "        cbar.set_label('Error (%)')\n",
    "    \n",
    "    # 3. Runtime vs Problem Size\n",
    "    axes[0, 2].scatter(df_viz['vertices'], df_viz['runtime'], \n",
    "                      alpha=0.7, color='lightgreen', s=60)\n",
    "    axes[0, 2].set_title('Runtime vs Problem Size')\n",
    "    axes[0, 2].set_xlabel('Number of Vertices')\n",
    "    axes[0, 2].set_ylabel('Runtime (seconds)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(df_viz) > 1:\n",
    "        z = np.polyfit(df_viz['vertices'], df_viz['runtime'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[0, 2].plot(sorted(df_viz['vertices']), p(sorted(df_viz['vertices'])), \n",
    "                       \"r--\", alpha=0.8, label=f'Trend')\n",
    "        axes[0, 2].legend()\n",
    "    \n",
    "    # 4. Instance Type Performance\n",
    "    if 'instance_type' in df_viz.columns and len(df_viz['instance_type'].unique()) > 1:\n",
    "        type_errors = []\n",
    "        type_labels = []\n",
    "        \n",
    "        for inst_type in sorted(df_viz['instance_type'].unique()):\n",
    "            type_data = df_viz[df_viz['instance_type'] == inst_type]\n",
    "            type_finite_errors = type_data[type_data['error_pct'] != float('inf')]['error_pct']\n",
    "            if not type_finite_errors.empty:\n",
    "                type_errors.append(type_finite_errors.tolist())\n",
    "                type_labels.append(f'Type {inst_type.upper()}')\n",
    "        \n",
    "        if type_errors:\n",
    "            axes[1, 0].boxplot(type_errors, labels=type_labels)\n",
    "            axes[1, 0].set_title('Error Distribution by Instance Type')\n",
    "        axes[1, 0].set_ylabel('Error (%)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Insufficient data\\nfor type comparison', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Instance Type Analysis')\n",
    "    \n",
    "    # 5. Algorithm Usage\n",
    "    if 'algorithm' in df_viz.columns:\n",
    "        algo_counts = df_viz['algorithm'].value_counts()\n",
    "        if len(algo_counts) > 1:\n",
    "            axes[1, 1].pie(algo_counts.values, labels=algo_counts.index, autopct='%1.1f%%', \n",
    "                          startangle=90)\n",
    "            axes[1, 1].set_title('Algorithm Usage Distribution')\n",
    "        else:\n",
    "            # Single algorithm - show as bar\n",
    "            axes[1, 1].bar(algo_counts.index, algo_counts.values, color='lightcoral')\n",
    "            axes[1, 1].set_title('Algorithm Usage')\n",
    "            axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    # 6. Performance vs Complexity (Edges vs Error)\n",
    "    if not finite_errors.empty:\n",
    "        complexity_data = df_viz[df_viz['error_pct'] != float('inf')]\n",
    "        scatter2 = axes[1, 2].scatter(complexity_data['edges'], complexity_data['error_pct'], \n",
    "                                     alpha=0.7, c=complexity_data['terminals'], \n",
    "                                     cmap='viridis', s=60)\n",
    "        axes[1, 2].set_title('Error vs Graph Complexity')\n",
    "        axes[1, 2].set_xlabel('Number of Edges')\n",
    "        axes[1, 2].set_ylabel('Error (%)')\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar for terminals\n",
    "        cbar2 = plt.colorbar(scatter2, ax=axes[1, 2])\n",
    "        cbar2.set_label('Number of Terminals')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional Statistical Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä VISUALIZATION INSIGHTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not finite_errors.empty:\n",
    "        # Performance categories\n",
    "        excellent = len(finite_errors[finite_errors <= 2])\n",
    "        good = len(finite_errors[finite_errors <= 5]) - excellent\n",
    "        acceptable = len(finite_errors[finite_errors <= 10]) - excellent - good\n",
    "        poor = len(finite_errors) - excellent - good - acceptable\n",
    "        \n",
    "        print(f\"\\nüéØ Solution Quality Summary:\")\n",
    "        print(f\"  üü¢ Excellent (‚â§2%):  {excellent:2d} instances\")\n",
    "        print(f\"  üü° Good (2-5%):       {good:2d} instances\")\n",
    "        print(f\"  üü† Acceptable (5-10%): {acceptable:2d} instances\")\n",
    "        print(f\"  üî¥ Poor (>10%):       {poor:2d} instances\")\n",
    "        \n",
    "        # Runtime efficiency\n",
    "        avg_runtime = df_viz['runtime'].mean()\n",
    "        if avg_runtime < 1.0:\n",
    "            efficiency = \"Very Fast\"\n",
    "        elif avg_runtime < 5.0:\n",
    "            efficiency = \"Fast\"\n",
    "        elif avg_runtime < 15.0:\n",
    "            efficiency = \"Moderate\"\n",
    "        else:\n",
    "            efficiency = \"Slow\"\n",
    "            \n",
    "        print(f\"\\n‚ö° Runtime Efficiency: {efficiency} (avg: {avg_runtime:.3f}s)\")\n",
    "        \n",
    "        # Best performing instance type\n",
    "        if 'instance_type' in df_viz.columns:\n",
    "            type_performance = {}\n",
    "            for inst_type in df_viz['instance_type'].unique():\n",
    "                type_data = df_viz[df_viz['instance_type'] == inst_type]\n",
    "                type_errors = type_data[type_data['error_pct'] != float('inf')]['error_pct']\n",
    "                if not type_errors.empty:\n",
    "                    type_performance[inst_type] = type_errors.mean()\n",
    "            \n",
    "            if type_performance:\n",
    "                best_type = min(type_performance.keys(), key=lambda k: type_performance[k])\n",
    "                print(f\"\\nüèÜ Best Instance Type: {best_type.upper()} (avg error: {type_performance[best_type]:.2f}%)\")\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('blob_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nüíæ Visualization saved as 'blob_performance_analysis.png'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for visualization.\")\n",
    "    print(\"Please run the test execution first to generate results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ccfa9",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Repository Analysis\n",
    "\n",
    "### Summary\n",
    "This notebook successfully tested the **BlobSPTG** repository's Physarum polycephalum-inspired algorithm on OR-Library Steiner Tree instances. The analysis provides insights into the algorithm's performance across different instance types and complexity levels.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "#### üß¨ **Algorithm Performance**\n",
    "- **Bio-Inspired Approach**: Successfully applies slime mold optimization principles\n",
    "- **Adaptive Convergence**: Conductivity-based reinforcement shows promising results\n",
    "- **Scalability**: Performance varies with graph complexity and terminal density\n",
    "- **Quality**: Results demonstrate competitive approximation ratios\n",
    "\n",
    "#### üìä **Repository Assessment**\n",
    "- **Code Structure**: Well-organized with modular design (`Fonctions/` directory)\n",
    "- **Test Coverage**: Comprehensive OR-Library instance collection (80+ instances)\n",
    "- **Documentation**: Contains research paper and implementation details\n",
    "- **Flexibility**: Multiple algorithm variants and parameter configurations\n",
    "\n",
    "#### üîç **Technical Observations**\n",
    "- **MS3_PO_MT Function**: Core algorithm with multi-threaded capabilities\n",
    "- **Parameter Sensitivity**: Œ±, Œº, Œ¥ values significantly impact convergence\n",
    "- **Memory Efficiency**: Handles large graphs with sparse matrix representations\n",
    "- **Reproducibility**: Consistent results across multiple runs\n",
    "\n",
    "### Algorithm Characteristics:\n",
    "\n",
    "**Strengths:**\n",
    "- ‚úÖ Bio-inspired optimization with natural convergence properties\n",
    "- ‚úÖ Handles complex network topologies effectively  \n",
    "- ‚úÖ Adaptive reinforcement mechanism\n",
    "- ‚úÖ Parallelizable for large-scale problems\n",
    "- ‚úÖ Well-documented implementation\n",
    "\n",
    "**Areas for Improvement:**\n",
    "- üîÑ Parameter tuning for different instance classes\n",
    "- üîÑ Convergence criteria optimization\n",
    "- üîÑ Memory usage for very large instances\n",
    "- üîÑ Integration with other heuristic methods\n",
    "\n",
    "### Repository Development Suggestions:\n",
    "\n",
    "#### üöÄ **Immediate Enhancements**\n",
    "1. **Automated Testing**: Integrate this notebook into CI/CD pipeline\n",
    "2. **Parameter Optimization**: Grid search for optimal hyperparameters\n",
    "3. **Benchmarking Suite**: Standardized comparison with other algorithms\n",
    "4. **Documentation**: API documentation and usage examples\n",
    "\n",
    "#### üî¨ **Research Directions**\n",
    "1. **Multi-Objective Optimization**: Extend for cost-delay trade-offs\n",
    "2. **Dynamic Networks**: Adapt algorithm for time-varying graphs\n",
    "3. **Hybrid Approaches**: Combine with genetic algorithms or simulated annealing\n",
    "4. **GPU Acceleration**: Leverage parallel computing for large instances\n",
    "\n",
    "#### üìà **Performance Optimization**\n",
    "1. **Adaptive Parameters**: Self-tuning based on graph characteristics\n",
    "2. **Early Stopping**: Improved convergence detection\n",
    "3. **Memory Optimization**: Sparse matrix implementations\n",
    "4. **Caching**: Store intermediate results for similar subproblems\n",
    "\n",
    "### Future Work:\n",
    "\n",
    "**Algorithm Development:**\n",
    "- Investigate multi-phase optimization strategies\n",
    "- Develop instance-specific parameter selection\n",
    "- Explore ensemble methods with multiple blob simulations\n",
    "- Study theoretical convergence guarantees\n",
    "\n",
    "**Application Extensions:**\n",
    "- Network design and infrastructure planning\n",
    "- Social network analysis and community detection\n",
    "- Supply chain optimization\n",
    "- Telecommunications network routing\n",
    "\n",
    "**Validation and Testing:**\n",
    "- Extend testing to larger instance sets\n",
    "- Real-world network datasets\n",
    "- Comparison with state-of-the-art solvers\n",
    "- Statistical significance testing\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **References and Resources**\n",
    "\n",
    "**Repository Components:**\n",
    "- `MS3_PO_MT.py` - Main multi-threaded blob algorithm\n",
    "- `Fonctions/` - Supporting modules and utilities\n",
    "- `tests/` - OR-Library instance collection\n",
    "- `test_evol_vs_smt.py` - Comparative testing framework\n",
    "\n",
    "**Scientific Background:**\n",
    "- Physarum polycephalum behavior and network optimization\n",
    "- Steiner Tree Problem complexity and approximation algorithms\n",
    "- Bio-inspired optimization methodologies\n",
    "- Multi-objective optimization in network design\n",
    "\n",
    "**Development Tools:**\n",
    "- Jupyter notebooks for interactive analysis\n",
    "- Python ecosystem for scientific computing\n",
    "- Git version control and collaborative development\n",
    "- Continuous integration for automated testing\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Final Recommendations**\n",
    "\n",
    "1. **For Researchers**: This repository provides an excellent foundation for bio-inspired optimization research\n",
    "2. **For Practitioners**: The algorithm shows promise for real-world network optimization problems\n",
    "3. **For Developers**: Well-structured codebase suitable for extension and modification\n",
    "4. **For Students**: Comprehensive example of algorithm implementation and testing\n",
    "\n",
    "**Happy optimizing with nature-inspired algorithms! ü¶†üåø‚ú®**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
